
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
 <channel>
   <title>Jiahai Feng</title>
   <link>https://jiahai-feng.github.io/</link>
   <description>Recent content on Jiahai Feng</description>
   <generator>Hugo -- gohugo.io</generator>
   <language>en-us</language>
   <lastBuildDate>Tue, 22 Feb 2022 00:00:00 +0000</lastBuildDate>
   
       <atom:link href="https://jiahai-feng.github.io/index.xml" rel="self" type="application/rss+xml" />
   
   
     <item>
       <title>Simpson&#39;s paradox</title>
       <link>https://jiahai-feng.github.io/posts/simpsons-paradox/</link>
       <pubDate>Wed, 10 May 2023 22:49:39 -0700</pubDate>
       
       <guid>https://jiahai-feng.github.io/posts/simpsons-paradox/</guid>
       <description>&lt;h2 id=&#34;or-why-statistics-is-scary&#34;&gt;or Why Statistics is Scary&lt;/h2&gt;&lt;p&gt;Simpson&amp;rsquo;s Paradox is something I&amp;rsquo;ve come across here and there, but I never fully understood its implications until Peng Ding&amp;rsquo;s Causal Inference class. Once I did, I became a lot less trusting of statistical correlations.&lt;/p&gt;&lt;h2 id=&#34;the-paradox-in-examples&#34;&gt;The Paradox in Examples&lt;/h2&gt;&lt;p&gt;There are two real-life examples of Simpson&amp;rsquo;s Paradox that Peng Ding went through. They are both present in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox#Examples&#34;&gt;wikipedia page&lt;/a&gt;. Here is the gist:&lt;/p&gt;&lt;h3 id=&#34;case-1-kidney-stone-treatments&#34;&gt;Case 1: Kidney stone treatments&lt;/h3&gt;&lt;p&gt;A study was conducted to compare the effectiveness of two treatments for kidney stones. The study looked at historical clinical data of people who underwent either treatment and computed their success rates. Treatment A was more aggressive than treatment B, and the doctors expected treatment A to be more effective. However, the study found that overall, people who underwent treatment A actually had lower success rates than people who underwent treatment B. What gives?&lt;/p&gt;&lt;p&gt;It turned out that the severity of the kidney stones affected whether doctors prescribed treatment A or treatment B. Patients who underwent treatment A tended to have more challenging conditions to begin with, and thus ended up having lower success rates. The &amp;ldquo;correct&amp;rdquo; way (based on contextual medical knowledge) to interpret the data was to first condition on disease severity, and then evaluate treatment effectiveness.&lt;/p&gt;&lt;h3 id=&#34;case-2-uc-berkeley-graduate-admissions&#34;&gt;Case 2: UC Berkeley Graduate Admissions&lt;/h3&gt;&lt;p&gt;The 1973 admissions data for the graduate schools of UC Berkeley was studied, and researchers found that women had a far lower admission rate than men. The difference was absurd (35% for women vs 44% for men). Here, admission rate simply referred to the proportion of applicants of that gender that was admitted. Reading the results of this study today, we are probably not surprised that there could be significant systematic bias against women, especially in the old days of 1973.&lt;/p&gt;&lt;p&gt;Interestingly, when researchers tried to identify the offending departments at UC Berkeley most responsible for the bias, they could not find any department that significantly exhibited bias against women - in fact, they found some statistical evidence that on the departmental level, some departments exhibited a small bias against men. Here, all they were doing different from the initial analysis was to look at the admission rates at a per department level. What gives?&lt;/p&gt;&lt;p&gt;It turned out that women, more than men, tended to apply to more selective departments. Even though individual departments were relatively unbiased, having more women apply to selective departments lowered the overall admission rates of women when aggregated over the entire school.&lt;/p&gt;&lt;h2 id=&#34;simpsons-paradox-in-the-abstract&#34;&gt;Simpson&amp;rsquo;s Paradox in the Abstract&lt;/h2&gt;&lt;p&gt;In general, suppose you have a real variable $Y$ that you care about (e.g. success rates of treatment or admission rates), and some covariate $X$ (e.g. treatment type or gender). The paradox occurs when you get one trend if you analyze the dependence of $Y$ on $X$, but a completely different one if you analyze the dependence of $Y$ on $X$, &lt;em&gt;conditioned on another covariate $Z$&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;Mathematically, in the abstract, it seemed obvious to me that the marginal distribution $P_{X, Y}(x, y)$ could look completely different from the conditioned distributions $P_{X, Y | Z}(x, y | z)$ for any choice of $z$. That is to say, I was not surprised by the mathematical statement of Simpson&amp;rsquo;s Paradox.&lt;/p&gt;&lt;p&gt;However, the practical implications were what I didn&amp;rsquo;t appreciate fully. What I realize now is that any time I have a set of joint data for two variables $X$ and $Y$, and I want to do some kind of regression to find the dependence of $Y$ on $X$, I should be extremely afraid of drawing any conclusion on the real relationship between $Y$ and $X$ that exists in the underlying data generating process.&lt;/p&gt;&lt;p&gt;The moral of the two stories presented above is that you should pay extreme attention to the control variables, because any variable you don&amp;rsquo;t control could end up confounding the results and possibly entirely negate your entire conclusion. In these two cases, we are fortunate to have domain knowledge to understand which variables are the &amp;ldquo;natural&amp;rdquo; variables to control and hence condition on when making statistical analyses. &lt;em&gt;The strength of the statistical conclusions are only as strong as the often implicit assumptions that go into modeling which variables are important enough to be controlled.&lt;/em&gt; In the admissions case, it seemed fitting to control for department, but what about other variables such as the level of preparation, aptitude, tenancity, or other qualities that a candidate might possess that make them a deserving admit? What if conditioning on these variables completely negate the gender bias conclusion again?&lt;/p&gt;&lt;h2 id=&#34;simpsons-paradox-in-ols&#34;&gt;Simpson&amp;rsquo;s Paradox in OLS&lt;/h2&gt;&lt;p&gt;Here&amp;rsquo;s a formulation of Simpson&amp;rsquo;s paradox in Ordinary Least Squares (OLS). Suppose you have a bunch of data $(X, Y, Z)$. You find that&lt;/p&gt;&lt;p&gt;$$Y \sim \beta_0 X + \alpha_0 Z + \epsilon$$&lt;/p&gt;&lt;p&gt;is the OLS fit. The $\beta_0$ you get here can be interpreted as the dependence of $Y$ on $X$ if we condition on $Z$. Contrast this with the $\beta_1$ you obtain from fitting&lt;/p&gt;&lt;p&gt;$$Y \sim \beta_1 X + \epsilon,$$&lt;/p&gt;&lt;p&gt;which you interpret as the aggregate dependence of $Y$ on $X$, without conditioning on $Z$.&lt;/p&gt;&lt;p&gt;It&amp;rsquo;s not difficult to imagine distributions of $(X, Y, Z)$ that lead to the Simpson&amp;rsquo;s paradox arising, i.e. having $\beta_0$ and $\beta_1$ have opposite signs. These two graphs from Wikipedia give good hints on how they might be constructed: &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Simpson%27s_paradox_continuous.svg/440px-Simpson%27s_paradox_continuous.svg.png&#34;&gt;Graph 1&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/File:Simpsons_paradox_-_animation.gif&#34;&gt;Graph 2&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&#34;simpsons-paradox-in-ols-every-where&#34;&gt;Simpson&amp;rsquo;s Paradox in OLS, every where&lt;/h2&gt;&lt;p&gt;I initially assumed that you need specially constructed distributions to exhibit Simpson&amp;rsquo;s paradox, but after thinking about it for a while, I realized that that &lt;em&gt;any distribution&lt;/em&gt; of $(X, Y, Z)$ can exhibit Simpson&amp;rsquo;s paradox if we get to choose which variable to control.&lt;/p&gt;&lt;p&gt;The setup is as follows:&lt;/p&gt;&lt;p&gt;Suppose, as above, I have data $(X, Y, Z)$ and I&amp;rsquo;m interested in how $Y$ depends on $X$. My expert friend tells me that $Z$ is the important variable to control for, and so I perform the OLS fit:$$Y \sim \beta_0 X + \alpha_0 Z + \epsilon$$&lt;/p&gt;&lt;p&gt;I show this to my other expert friend, who then says that $Z$ is the wrong variable to condition on. What I need to condition on is the variable $$Z&amp;rsquo; = Z + 2\frac{\beta_0}{\alpha_0}X.$$&lt;/p&gt;&lt;p&gt;Now, if I do the OLS fit$$ Y \sim \beta_2 X + \alpha_2 Z&amp;rsquo; + \epsilon,$$I will find that $\alpha_2 = \alpha_0$, but $\beta_0=-\beta_2$. (Proof in footnotes${}^1.) $ The dependence of $Y$ on $X$, under a different set of control variables, has completely reversed! Simpson&amp;rsquo;s paradox${}^2$ thus lurks in every distribution, and we have to be &lt;strong&gt;constantly vigilant&lt;/strong&gt;!&lt;/p&gt;&lt;p&gt;In practice, if we suspect that any variable that we are unable to control for (perhaps because of experimental limitations) is strongly correlated with one of the covariates we&amp;rsquo;re studying, then we should be extremely cautious of any result that arises from statistical analyses - you might even say that the conclusions are useless.&lt;/p&gt;&lt;h2 id=&#34;philosophical-musings&#34;&gt;Philosophical musings&lt;/h2&gt;&lt;p&gt;Simpson&amp;rsquo;s paradox was presented in Peng Ding&amp;rsquo;s class to essentially motivate the entire field of Causal Inference. His presentation certainly shook me - I was aware that correlation does&amp;rsquo;t mean causation, but I didn&amp;rsquo;t know it is &lt;em&gt;this bad&lt;/em&gt;. In addition, the complete dependence of the OLS coefficient on the choice of control variable invalidates any study that does not systematically investigate every possible confounder and justify its choice of control variables. I am more mistrustful of observational studies and also have become more cautious when applying statistics in my own work.&lt;/p&gt;&lt;p&gt;Particularly in the study of &lt;em&gt;mechanistic interpretability&lt;/em&gt;, I often encounter complex deterministic systems. It&amp;rsquo;s extremely unclear in many situations where strictly speaking, everything is perfectly correlated${}^3$, to determine what are the correct variables to control, manipulate, or measure. Sometimes, I just make a choice and hope that it is good enough, but this exploration into Simpson&amp;rsquo;s paradox has made me more cautious about these decisions and how much I trust my results.&lt;/p&gt;&lt;p&gt;As a final word, I want to reiterate how scary doing statistics really is. In the two examples, we are fortunate to have domain expertise that gives some sense of what the &amp;ldquo;correct&amp;rdquo; control variables are or what the &amp;ldquo;correct&amp;rdquo; results should be. What if we don&amp;rsquo;t have domain expertise, or if our domain expertise contains false beliefs? To echo my takeaway from earlier:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The strength of the statistical conclusions are only as strong as the often implicit assumptions that go into modeling control variables.&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Here&amp;rsquo;s a quick proof. Ditch the above notation. Let $Y\in \mathbb{R}^{n\times 1}$, $X\in \mathbb{R}^{n\times d}$ be the data that we have with $n$ data points and $d$ dimensional covariates. Let $\beta\in \mathbb{R}^{d\times 1}$ be the minimizer for $||Y-X\beta||^2$. Then, for any invertible $M\in \mathbb{R}^{d\times d}$, $||Y-X\beta||^2 = ||Y-XMM^{-1}\beta||^2$. This means that the OLS solution for $(Y, XM)$ is $M^{-1} \beta$. Substitute relevant values of $M$ and we&amp;rsquo;re done.&lt;/li&gt;&lt;li&gt;Technically, Simpson&amp;rsquo;s paradox usually refers to reversing trends when taking aggregates vs conditioning on a specific variable, whereas here we reverse trends by conditioning on two different variables. However, you can easily recover the aggregate setting by changing the story so that the first expert friend recommends not conditioning on anything at all, and the second expert friend recommends conditioning on $$Z&amp;rsquo;&amp;rsquo; = Z + \frac{\beta_0 + \beta_1}{\alpha_0}X.$$&lt;/li&gt;&lt;li&gt;To muse even further, the world itself is a deterministic system (if you consider the wavefunction of the universe). What are the implicit assumptions about the world we&amp;rsquo;re making every time we do causal inference?&lt;/li&gt;&lt;/ol&gt;</description>
     </item>
   
     <item>
       <title>Cache Oblivious Algorithms</title>
       <link>https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/</link>
       <pubDate>Sun, 10 May 2020 22:49:39 -0700</pubDate>
       
       <guid>https://jiahai-feng.github.io/posts/cache-oblivious-algorithms/</guid>
       <description>&lt;h1 id=&#34;cache-oblivious-algorithms&#34;&gt;Cache-Oblivious Algorithms&lt;/h1&gt;&lt;p&gt;Cache-oblivious algorithms seemed incredible to me when I first heard about it in 6.854 (Advanced Algorithms). It&amp;rsquo;s relatively straightforward to imagine algorithms that utilizes information about page size $B$ and cache size $M$ to create efficient data structures. However, cache-oblivious algorithms are algorithms that achieve similar efficiencies &lt;em&gt;without knowing $B$ or $M$&lt;/em&gt;. That means the same cache-oblivious algorithm can be run on computers with different cache or page sizes and still achieve the same asymptotic efficiency as one that is tailored to that specific cache. The key to many of these algorithms is a recursive, fractal-like idea. I&amp;rsquo;ll describe a cache-oblivious algorithm for searching in a list of integers.&lt;/p&gt;&lt;h1 id=&#34;whats-a-cache---the-external-memory-model&#34;&gt;What&amp;rsquo;s a cache? - The External Memory Model&lt;/h1&gt;&lt;p&gt;Here&amp;rsquo;s the set up. (TLDR at bottom) A program executes on the processor, but the input data resides on the disk. We can think of a disk as a really long linear array of data, but accessing it is really expensive. So, we instead invent this notion of a cache - a layer of memory of $M$ bits that is much smaller, but a lot faster to read/write to. For performance reasons, we will divide the disk and cache into blocks of $B$ bits, called pages, and we will transfer data between cache and disk a page at a time.&lt;/p&gt;&lt;p&gt;This cache sits between the processor and the disk, and the processor will never directly read or write to the disk. When the processor wants to read a piece of data that is not on the cache (this is called a cache miss), it will trigger some mechanism that copies the relevant page from the disk to the cache first, &lt;em&gt;evicting another page if the cache is full&lt;/em&gt;, and then read from the cache.&lt;/p&gt;&lt;p&gt;How might this help? Well, trivially, if the entire set of data that the program needs fits in the cache, conceivably the processor could have fetched it all in the cache, paid the one-off cost, and then for the rest of the program the processor will never have to pay the expensive cost of reading from disk. Of course most programs require more data than that, and so we want to design algorithms that minimize cache misses.&lt;/p&gt;&lt;h2 id=&#34;cache-management&#34;&gt;Cache management&lt;/h2&gt;&lt;p&gt;However, the way computers are set up restricts the tools we have to achieve this. Programs executing on the processor have no explicit control over the cache. Instead, conceptually, programs continue to send read and write requests to the disk as though there is no cache. The processor intercepts these requests, and performs the necessary cache operations to serve the information. In other words, the cache is transparent to programs.  Thus, the task of reducing cache misses decomposes into two orthogonal components: writing efficient algorithms by cleverly ordering or specifying memory accesses, and efficient cache management.&lt;/p&gt;&lt;p&gt;For example, an efficient algorithm might try to shuffle memory accesses around so that memory accesses to a particular memory address are clumped together, increasing the likelihood that there will be a cache hit. In fact, because of the paging system, it would doubly help if memory accesses to the same &lt;em&gt;spatial region&lt;/em&gt; are clumped together, because there&amp;rsquo;s a chance they&amp;rsquo;ll be on the same page.&lt;/p&gt;&lt;p&gt;On the other hand, efficient cache management entails trying to predict which piece of data to evict from the cache when there is a cache miss and we want to load more data into an already full cache. Since we&amp;rsquo;re focusing on the algorithm this article, we assume that the cache manager has magic oracle powers that lets it discard &lt;em&gt;optimally&lt;/em&gt;, so from the algorithm design standpoint, if there exists a sequence of cache loads/writes that achieves our desired performance, we assume that the cache manager will do it. It turns out that this is not unreasonable, because simple techniques such as the Least Recently Used algorithm work &lt;em&gt;really&lt;/em&gt; well. (An exposition on this deserves its own article.)&lt;/p&gt;&lt;p&gt;To summarize, the components of the external memory model are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Disk, effectively unbounded in size, stores data in pages of size $B$&lt;/li&gt;&lt;li&gt;Cache, stores $M/B$ pages, each page has size $B$&lt;/li&gt;&lt;li&gt;Processor executes algorithm, also manages cache.&lt;/li&gt;&lt;li&gt;Algorithm runs on the processor. When data is needed, the processor transparently loads the required page into the cache if there&amp;rsquo;s a cache miss, or serve the cached data if there isn&amp;rsquo;t&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;There are efficient algorithms that require explicit knowledge of $B$ and $M$, and there cache-oblivous algorithms that are just as efficient, but some how do not. That is to say, the algorithm performs the same actions and requests the same sequence of memory accesses, that works efficiently no matter what size $B$ and $M$ really are. We&amp;rsquo;ll take a close look at the latter here.&lt;/p&gt;&lt;h1 id=&#34;a-trivial-example-scanning&#34;&gt;A trivial example: scanning&lt;/h1&gt;&lt;p&gt;Let&amp;rsquo;s say we want to scan through a list of $N$ integers, to do some task, say finding the maximum. If the list occupy a contiguous block of memory on the disk, this can be done efficiently by loading pages one at a time, and scanning through the page before loading the next page. This thus takes $O(N/B)$ disk accesses, and is quite clearly optimal.&lt;/p&gt;&lt;p&gt;Suppose the algorithm does not know $B$. It can achieve the same efficiency by simply reading the elements of the list in order. The CPU will then fetch the first page to the cache, containing elements $0$ to $B-1$ and pass it to the program, and then when the program requires element $B$, it&amp;rsquo;ll read in the next page, which contains elements $B$ to $2B-1$, possibly overwriting the earlier page. This will just continue until all $N/B$ pages are read. This is cache-oblivious, because the algorithm of sequentially reading the elements is the same regardless of what $B$ is.&lt;/p&gt;&lt;p&gt;Note that in this example, it is essential that the list is stored in a contiguous block of memory. Had the elements been scattered around like in a linked-list, each access could potentially trigger a cache miss, since there&amp;rsquo;s no guarantee that neighboring elements will be on the same page. This may seem obvious, but this is the main mechanism by which we can write efficient algorithms - coming up with efficient layouts of data.&lt;/p&gt;&lt;h1 id=&#34;a-puzzle-searching&#34;&gt;A puzzle: Searching&lt;/h1&gt;&lt;p&gt;Now, let&amp;rsquo;s consider the problem of searching for an integer in a list. We&amp;rsquo;ll examine three ways of doing this.&lt;/p&gt;&lt;h2 id=&#34;inefficient-cache-oblivious-algorithm-binary-search&#34;&gt;Inefficient cache-oblivious algorithm: Binary search&lt;/h2&gt;&lt;p&gt;Let&amp;rsquo;s assume the data is sorted in a list, and packed contiguously on the disk. A possible algorithm is then to perform binary search, which typically takes $O(\log N)$ reads. The binary search can be viewed a successively narrowing an interval in which the query value may reside in. However, once the interval that the binary search is narrowing is $O(B)$, the entire interval may reside in a page, and no further page accesses are required. Thus, we require only $O(\log N - \log B)$ page accesses. Notice that this is a cache-oblivious algorithm because the algorithm doesn&amp;rsquo;t know what $B$ is - to the algorithm, it&amp;rsquo;s just running a standard binary search.&lt;/p&gt;&lt;h2 id=&#34;efficient-but-not-cache-oblivious-b-trees&#34;&gt;Efficient, but not cache-oblivious: B-trees&lt;/h2&gt;&lt;p&gt;Suppose we know $B$ ahead of time. We can achieve $O(\log_B N)$ page accesses. We arrange the list in the format of a B-tree, a generalized BST. Similar to a BST, each subtree of a B-tree corresponds to a contiguous subset of the elements of the list when sorted by value. However, each node $V$ of the tree contains not one, but $B$ elements, which splits the interval corresponding to the subtree rooted at node $V$ into not 2, but $B+1$ intervals. Then, we can traverse down the BST, and each time we go down one level we reduce the interval by a factor of $B$. This thus requires only $O(\log_B N) = O(\log N / \log B)$ page accesses. In reality of course, we have to store pointers and possibly other metadata in each node, so we can&amp;rsquo;t have exactly $B$ elements in each node, but we can definitely split by $\Theta(B)$ each level.&lt;/p&gt;&lt;h2 id=&#34;efficient-cache-oblivious-algorithm-van-emde-boas-layout&#34;&gt;Efficient cache-oblivious algorithm: van Emde Boas layout&lt;/h2&gt;&lt;p&gt;Now we come to the exciting part. We claim that we can achieve $O(\log_B N)$ page accesses, but without having to know $B$ ahead of time. The data structure we&amp;rsquo;re using is a good old balanced Binary Search Tree. However, the order in which we store the nodes in memory matters. If we pack the the BST in a regular fashion, say in breadth-first order, we could potentially have a situation where almost every vertex in a BST search path, except for the first few, lies in a different page, and thus we&amp;rsquo;ll have to use $O(\log N - \log B)$ page accesses. The van Embde Boas layout is basically a clever way of ordering the vertices of a binary search tree in a recursive, fractal-like manner such that each page access will fetch the next few vertices that will be queried, so that the next few accesses will be contained within that page.&lt;/p&gt;&lt;h3 id=&#34;concise-definition&#34;&gt;Concise definition&lt;/h3&gt;&lt;figure&gt;&lt;img src=&#34;https://jiahai-feng.github.io/images/van-embde-boas-layout-labelled.PNG&#34;         alt=&#34;One application of the recursive van Embde Boas layout&#34;/&gt;&lt;figcaption&gt;            &lt;p&gt;One application of the recursive van Embde Boas layout&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For convenience, suppose the binary tree is complete and has height $H=2^K$. The layout is a permutation of the vertices of the binary tree. Denote $T(x, k)$ as the subtree of height $2^k$, rooted at vertex $x$. If the subtree at $x$ is deeper than $2^k$, then $T(x,k)$ will be the subtree truncated to the required depth. Denote $L(x, k)$ as the layout for that subtree. Then, for some root vertex $x$, of a (sub-)tree with height $h=2^k$, $L(x, k)$ is defined recursively as:&lt;/p&gt;&lt;p&gt;$$L(x, k) = L(x, k-1) \circ L(l_1, k-1) \circ L(l_2, k-1) \circ \dots \circ L(l_{n}, k-1)$$&lt;/p&gt;&lt;p&gt;Where, $l_1, \dots, l_{n}$ are the $n=2^{h/2}$ children of the leaves of $T(x, k-1)$, and $\circ$ is just concatenation. One application of this recursion is shown above.&lt;/p&gt;&lt;h3 id=&#34;visual-sketch&#34;&gt;Visual sketch&lt;/h3&gt;&lt;p&gt;If you didn&amp;rsquo;t catch that, don&amp;rsquo;t worry about it. Here&amp;rsquo;s a walkthrough. Suppose we&amp;rsquo;re given a complete binary tree of height $h=2^k$, rooted at $x$. We want to compute $L(x, k)$, which is a permutation of the vertices in this binary tree. Here&amp;rsquo;s how it&amp;rsquo;s done:&lt;/p&gt;&lt;figure&gt;&lt;img src=&#34;https://jiahai-feng.github.io/images/veb-step0.PNG&#34;         alt=&#34;Initial tree&#34;/&gt;&lt;figcaption&gt;            &lt;p&gt;Initial tree&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;ol&gt;&lt;li&gt;Cut all the edges between the $2^{k-1}$-th and $2^{k-1}+1$-th layer. This gives us:&lt;ul&gt;&lt;li&gt;$2^{h/2}$ smaller BSTs with height $2^{k-1}$. These are the children subtrees now disconnected from the main subtree, rooted at vertices $l_1$ to $l_n$, where $n=2^{h/2}$&lt;/li&gt;&lt;li&gt;One more BST with height $2^{k-1}$, which is the what&amp;rsquo;s left of the main subtree, now truncated in depth&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img src=&#34;https://jiahai-feng.github.io/images/veb-step1.PNG&#34;         alt=&#34;Partition tree&#34;/&gt;&lt;figcaption&gt;            &lt;p&gt;Partition tree&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start=&#34;2&#34;&gt;&lt;li&gt;Apply recursive procedure $L(\cdot, k-1)$ to obtain layouts for each of the $n + 1$ BSTs&lt;/li&gt;&lt;li&gt;Concatenate the layouts and return (the order doesn&amp;rsquo;t actually matter)&lt;figure&gt;&lt;img src=&#34;https://jiahai-feng.github.io/images/veb-step2.PNG&#34;         alt=&#34;Concatenate sub-layouts&#34;/&gt;&lt;figcaption&gt;            &lt;p&gt;Concatenate sub-layouts&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&#34;sierpinski-triangles-atoms-and-time&#34;&gt;Sierpinski triangles, atoms and time&lt;/h3&gt;&lt;p&gt;Now this tells us how to construct the layout, but it&amp;rsquo;ll be more helpful to develop some intuition about what this layout looks like.&lt;/p&gt;&lt;p&gt;An intuitive way to look at this is to draw analogy to the Sierpinski triangle, which can be constructed by repeatedly applying the same partitioning procedure to a full triangle. We can view successive partitioning as an evolution in time. Similar to the Sierpinski triangle, the layout partitions the subtree into smaller subtrees, and then recursively applies the same procedure to each smaller subtree.&lt;/p&gt;&lt;figure&gt;&lt;img src=&#34;https://jiahai-feng.github.io/images/sierpinski-triangle-evolution.png&#34;         alt=&#34;Evolution of Sierpinski Triangle (Wikimedia Commons)&#34;/&gt;&lt;figcaption&gt;            &lt;p&gt;Evolution of Sierpinski Triangle (Wikimedia Commons)&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This analogy gives us some insights. Recursion is typically viewed as a depth-first process, as most programming languages implement it that way. This leads us to instinctively follow a recursion branch deeper and deeper, but this doesn&amp;rsquo;t really give us the intuition we need to understand this. Instead, we can adopt the perspective that the Sierpinsiki triangle gives, and we instead &lt;em&gt;limit the recursion depth&lt;/em&gt;. This is akin to expanding the recursion in a &lt;em&gt;breadth-first&lt;/em&gt; rather than a &lt;em&gt;depth-first&lt;/em&gt; manner.&lt;/p&gt;&lt;p&gt;To put things concretely, let $t$ be the recursion depth limit. We introduce a term &amp;lsquo;atom&amp;rsquo; for the subtree unit that the recursion partitions into at depth $t$. For $t=0$, the recursion is completely unexpanded, and the atom is the whole tree. Quite trivially, all vertices in this atom are placed in a contiguous range in the final layout.&lt;/p&gt;&lt;figure&gt;&lt;img src=&#34;https://jiahai-feng.github.io/images/veb-step0.PNG&#34;         alt=&#34;Recursion depth is 0. The atom is the whole tree&#34;/&gt;&lt;figcaption&gt;            &lt;p&gt;Recursion depth is 0. The atom is the whole tree&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, to step into $t=1$, we evaluate one layer of the recursion. The atoms are now subtrees of height $2^{k-1}$. The concatenation process joins the layouts of the individual atoms, so we can safely say that in the final layout, all vertices in the same atom are in a contiguous segment. Notice that for this property to hold, it doesn&amp;rsquo;t matter which order we joined the layouts of the atoms in as long as we don&amp;rsquo;t break an atom up.&lt;/p&gt;&lt;figure&gt;&lt;img src=&#34;https://jiahai-feng.github.io/images/veb-step1.PNG&#34;         alt=&#34;Recursion depth is 1. The atoms are subtrees of height $2^{k-1}$&#34;/&gt;&lt;figcaption&gt;            &lt;p&gt;Recursion depth is 1. The atoms are subtrees of height $2^{k-1}$&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now, we evaluate one more layer of the recursion for $t=2$. The atoms are now subtrees of height $2^{k-2}$. For the same reasoning as before, in the final layout, all vertices in the same atom are in a contiguous segment.&lt;/p&gt;&lt;figure&gt;&lt;img src=&#34;https://jiahai-feng.github.io/images/veb-recur.PNG&#34;         alt=&#34;Recursion depth is 2. The atoms are subtrees of height $2^{k-2}$&#34;/&gt;&lt;figcaption&gt;            &lt;p&gt;Recursion depth is 2. The atoms are subtrees of height $2^{k-2}$&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And this process continues until the atoms are individual vertices.Thus, if we limit the recursion depth, we can view the recursive calls to the layout as successively partioning the tree into finer and finer atoms. More precisely, in the evolution for the van Embde Boas layout, after applying $t$ levels of recursive calls, all vertices are partitioned into subtrees of height $2^{K-t}$. This perspective lets us say something powerful about the atoms:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;In the final layout, we can choose any resolution to look at the layout. All vertices in any atom at this resolution will be in a contiguous segment.&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;&lt;p&gt;At this point, you might already have a pretty good idea of what the algorithm is. Given this layout, the cache-oblivious algorithm is the straightforward algorithm that traverses down this BST. Now, let&amp;rsquo;s analyze more formally why this is efficient.&lt;/p&gt;&lt;p&gt;Suppose the page size is $B$. Every time step, the height of atoms halves. We&amp;rsquo;re interested in the height of the atoms at the first time step where an entire atom can fit in a page. Since the number of vertices in a complete binary tree grows exponentially with height, that happens when atoms have height $\Theta(\log B)$. Then, analysing the layout at this resolution, we can fit any atom (which all have the same height of $\Theta(\log B)$) into the cache with 1 page load*.&lt;/p&gt;&lt;p&gt;Then, now consider what happens in a search. A search basically consists of a path from the root of the BST to some leaf*. This path will spend some time in the first atom, until it reaches the leaf of the atom and goes into the next atom, and so forth. Since the path always start at the root vertex of an atom and ends on a leaf, it will spend $\Theta(\log B)$ steps in that atom. Since the overall search path is $\log N$ steps long, we&amp;rsquo;ll need $O(\frac{\log N}{\log B}) = O(\log_B N)$ atoms, and that&amp;rsquo;s the number of page accesses we need.&lt;/p&gt;&lt;h2 id=&#34;remarks&#34;&gt;Remarks&lt;/h2&gt;&lt;p&gt;The heart of the algorithm is the fractal nature of the van Embde Boas layout, which gives some kind of scale invariance. This idea of geometric scaling is actually a pretty common argument in algorithm analysis, but I find this particular instance incredibly visual and vivid. I was guilty of cutting a few corners here, and I&amp;rsquo;ve marked some of the more egregious parts with an asterix. Nonetheless, I hope I was able to convey the main intuition in this article and if you&amp;rsquo;re interested in the details I glossed over or reading more about cache-oblivious algorithms, I&amp;rsquo;d recommend Erik Demaine&amp;rsquo;s awesome and accessible &lt;a href=&#34;http://erikdemaine.org/papers/BRICS2002/paper.pdf&#34;&gt;review paper&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Edit: &lt;a href=&#34;https://news.ycombinator.com/item?id=23662434&#34;&gt;hacker news discussion&lt;/a&gt;&lt;/p&gt;</description>
     </item>
   
 </channel>
</rss>
